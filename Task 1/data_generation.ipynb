{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import neseccary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ihorkostiuk/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "from html.parser import HTMLParser\n",
    "import urllib.request\n",
    "from deep_translator import GoogleTranslator\n",
    "import nltk\n",
    "import json\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "\n",
    "nltk.download('punkt')\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_row', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = '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'\n",
    "openai.api_key = API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, there I have not found any suitable dataset for this task so far. To overcome this issue I will generate dataset on my own, using API to chatGPT. The main idea is to extract all popular names of the mountains and pass to them chat GPT, to generate sentences. Also it has to be said, that ChatGPT has strong biases and can generate very similar results (duplicates), that can greatly influence the quality of the dataset.\n",
    "\n",
    "To resolve this issue, I will also gather decent amount of data from articles written by people (or I think, that they are written by people, who knows). They will be included in the resulting dataset.\n",
    "\n",
    "At the end we will have three columns in our dataset: Sentence, Tags, Source. Source means whether it was human written, gpt-generated (so there will be two classes: HUMAN, GPT). Also tags will be B-mount, I-mount and O."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./misc/Data Generation.jpg'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting mountain names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, to generated desirable sntences we need to consider extracting all known mountain names around the world. To accomplish this, we will define custom HTML-parser and parse List of all mountains on Wikipedia. In the end we will have 1500 names of the most popular mountains!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomParser(HTMLParser):\n",
    "    def __init__(self, dataset: list, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.in_table = False\n",
    "        self.first = False\n",
    "        self.dataset = dataset\n",
    "    \n",
    "    \n",
    "    def handle_data(self, data: str) -> None:\n",
    "        if self.in_table and self.first:\n",
    "            if data.strip() != '':\n",
    "                self.dataset.append(data)\n",
    "                self.first = False\n",
    "\n",
    "\n",
    "    def handle_starttag(self, tag: str, attrs: list[tuple[str, str | None]]) -> None:\n",
    "        if tag == 'tbody':\n",
    "            self.in_table = True\n",
    "        \n",
    "        if tag == 'tr' and self.in_table:\n",
    "            self.first = True\n",
    "\n",
    "\n",
    "    def handle_endtag(self, tag: str) -> None:\n",
    "        if tag == 'tbody':\n",
    "            self.in_table = False\n",
    "\n",
    "\n",
    "def extract_webpage(url: str) -> str:\n",
    "    fp = urllib.request.urlopen(url=url)\n",
    "    mybytes = fp.read()\n",
    "\n",
    "    mystr = mybytes.decode('utf8')\n",
    "    fp.close()\n",
    "\n",
    "    return mystr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting all names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Along the way I will encounter also non-ascii symbols, we will transliterate them using 'transliterate' python package. Also some of the mountain names on the wikipedia page have something included in '()', which can be an alternative name, or nearby location, so I will not include them in the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mountains_page = 'https://en.wikipedia.org/wiki/List_of_mountains_by_elevation'\n",
    "html_page = extract_webpage(url=mountains_page)\n",
    "test_data = []\n",
    "parser = CustomParser(dataset=test_data)\n",
    "parser.feed(html_page)\n",
    "\n",
    "translator = GoogleTranslator(source='auto', target='english')\n",
    "test_data = [name.split('(')[0].strip() for name in test_data]\n",
    "result_data = [name if name.isascii() else translator.translate(name) for name in test_data if name != 'Mountain']\n",
    "\n",
    "data = pd.DataFrame(result_data, columns=['Mountain'])\n",
    "data.to_csv('mountain_names.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating NER dataset using Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will use openai API, to generate sentences using ChatGPT. Then I will provide encoding for each of this sentences. To generate such sentences I will use from 1 to 3 names, which I will randomly extract from the dataset of names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/mountain_names.csv')\n",
    "system_role = ['adventurer', 'journalist', 'from National Geographic', 'author', 'scientist', 'climber', 'naturalist']\n",
    "actions = ['travveling around the globe', 'writing an article', 'describing what you see', 'expressing your feelengs', 'embracing nature']\n",
    "\n",
    "\n",
    "def ask_openai(prompt: str) -> str:\n",
    "    '''\n",
    "        This function sends request to the OpenAI API using system and user prompt.\n",
    "    '''\n",
    "    role = np.random.choice(system_role)\n",
    "    action = np.random.choice(actions)\n",
    "\n",
    "    print('Role: %s, action: %s' % (role, action))\n",
    "\n",
    "    system_content = 'You are {role} {action}.'.format(role=role, action=action)\n",
    "\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-1106\",\n",
    "        messages=[\n",
    "            {'role': 'system', 'content': system_content},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=512, # Unfortunately, I can't afford more.\n",
    "        temperature=0.5 # I am not concerned about truthfulness of statements, but rather trying to make bot as creative as possible. \n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "\n",
    "def get_random_names(names: pd.DataFrame) -> list:\n",
    "    indeces = np.random.choice(len(names), np.random.choice([1, 2, 3], p=[0.7, 0.2, 0.1]))\n",
    "\n",
    "    return names.loc[indeces]\n",
    "\n",
    "\n",
    "def log_json(json_str, file: str='./data/logs/log.txt'):\n",
    "    '''\n",
    "        This function is for saving strings in case something goes wrong.\n",
    "    '''\n",
    "    with open(file=file, mode='a') as f:\n",
    "        f.write(json_str + '\\n')\n",
    "\n",
    "\n",
    "def generating_loop(template: str=None, iterations: int=20, sentence_num: int=5, names_dataset: pd.DataFrame=None) -> pd.DataFrame:\n",
    "    '''\n",
    "        This function goes in a loop for iterations times. Each iteration generates its own sentences in quatity sentence_num.\n",
    "    Function returns pandas dataframe with columns: sentence, expected_tokens.\n",
    "    '''\n",
    "    result = []\n",
    "\n",
    "    template_2 = '''Using the following mountain names: {lists}. For each set of names generate sentence with them. Answer generate in JSON format with 'sentence', 'names' keywords.\n",
    "                    All mountain entities must be included in 'names'.'''\n",
    "\n",
    "    for itr_index in range(iterations):\n",
    "        temp_list = []\n",
    "        for sentence_index in range(sentence_num):\n",
    "            names = get_random_names(names_dataset)\n",
    "            names = names['Mountain'].values.tolist()\n",
    "            temp_list.append(names)\n",
    "        \n",
    "        prompt = template_2.format(lists=temp_list)\n",
    "\n",
    "        response = ask_openai(prompt=prompt)\n",
    "        try:\n",
    "            loaded_response = json.loads(response)\n",
    "            log_json(response)\n",
    "        except Exception as e:\n",
    "            print('Caught an exception: ', e)\n",
    "            continue\n",
    "        finally:\n",
    "            print(response)\n",
    "        \n",
    "\n",
    "        for mountain_dict in loaded_response['sentences']:\n",
    "            result.append(mountain_dict)\n",
    "\n",
    "    return pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for postprocessing responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_tags(names: list, sentence: str, dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    begin_t = 'B-mount'\n",
    "    inter_t = 'I-mount'\n",
    "    zero_t = 'O'\n",
    "    \n",
    "    expected_tokens = []\n",
    "    for name in names:\n",
    "        expected_tokens.extend(nltk.word_tokenize(name))\n",
    "\n",
    "    result = []\n",
    "    begin = True\n",
    "    for word in nltk.word_tokenize(sentence):\n",
    "        if word in expected_tokens:\n",
    "            if begin:\n",
    "                begin = False\n",
    "                result.append(begin_t)\n",
    "            else:\n",
    "                result.append(inter_t)\n",
    "        else:\n",
    "            begin = True\n",
    "            result.append(zero_t)\n",
    "\n",
    "    dataframe.loc[len(dataframe)] = [names, sentence, result]\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def add_entries(entries: str, dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    for entry_dict in entries['sentences']:\n",
    "        dataframe.loc[len(dataframe)] = [entry_dict['names'], entry_dict['sentence']]\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def load_logs(filename: str) -> str:\n",
    "    content = ''\n",
    "    with open(filename, 'r') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    return json.loads(content)\n",
    "\n",
    "\n",
    "def generate_tags(logs_filename: str) -> pd.DataFrame:\n",
    "    logs = load_logs(logs_filename)\n",
    "    dataset = pd.DataFrame(columns=['names', 'sentence'])\n",
    "    result = pd.DataFrame(columns=['names', 'sentence', 'tags'])\n",
    "\n",
    "    for log in logs:\n",
    "        dataset = add_entries(entries=log, dataframe=dataset)\n",
    "\n",
    "    for index in range(len(dataset)):\n",
    "        entry = dataset.loc[index]\n",
    "        result = append_tags(entry['names'], entry['sentence'], result)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities_openai(sentences):\n",
    "    example ='''{\n",
    "                    \"sentences\": [\n",
    "                        {\n",
    "                        \"names\": [\"Shiprock\"],\n",
    "                        \"sentence\": \"Shiprock rises majestically from the desert landscape, a striking solitary peak.\"\n",
    "                        },\n",
    "                        {\n",
    "                        \"names\": [\"Tödi\"],\n",
    "                        \"sentence\": \"Tödi, also known as Piz Russein, is the highest mountain in the Glarus Alps of Switzerland.\"\n",
    "                        }\n",
    "                    ]\n",
    "                }'''\n",
    "    \n",
    "    template = 'In the sentences list: {sentences} for each sentence find entities of mountain names, ranges and give them in JSON format like {example}, where names - is the entities, and sentence - is the sentence containing those entities.'\n",
    "\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-1106\",\n",
    "        messages=[\n",
    "            {'role': 'user', 'content': template.format(sentences=sentences, example=example)}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "    \n",
    "\n",
    "def extraction_log(response: str, filename: str='./data/logs/extraction_log_4.txt'):\n",
    "    with open(filename, mode='a') as f:\n",
    "        f.write(response + ',\\n')\n",
    "\n",
    "\n",
    "def generate_names_from_natural_sentence(filename: str, per_ask: int=10) -> pd.DataFrame:\n",
    "    sentences = []\n",
    "    with open(filename, mode='r') as f:\n",
    "        sentences = f.readlines()\n",
    "\n",
    "    sentences = [sentence.strip() for sentence in sentences]\n",
    "\n",
    "    for index in range(len(sentences) // per_ask):\n",
    "        sentences_to_send = sentences[index * per_ask : (index + 1) * per_ask]\n",
    "        response = extract_entities_openai(sentences=sentences_to_send)\n",
    "        print(response)\n",
    "        extraction_log(response=response)\n",
    "\n",
    "        sentences.append(response)\n",
    "\n",
    "    sentences_to_send = sentences[(index + 1) * per_ask : (index + 2) * per_ask]\n",
    "    if len(sentences_to_send) != 0:\n",
    "        response = extract_entities_openai(sentences=sentences_to_send)\n",
    "        print(response)\n",
    "        extraction_log(response=response)\n",
    "\n",
    "        sentences.append(response)\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating from the names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we generate 'log.txt' file, in which there will be sentences. We generate save all responces in the file due to problems with ChatGPT API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic = generating_loop(iterations=40, sentence_num=5, names_dataset=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we extract all generated data and tokenize it. Then we tag all sentences and construct a dataframe from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic = generate_tags('./data/logs/log.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save our final data to the '.csv' file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic.to_csv('./data/data_with_tags/synthetic_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting names from the natural sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "natural = generate_names_from_natural_sentence('./data/natural_samples_2.txt', per_ask=1) # Sent requests to the OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "natural = generate_tags('./data/logs/extraction_log_4.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "natural.to_csv('./data/data_with_tags/natural/natural_data_4.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging data from both sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_datasets(natural_folder: str, synthetic_folder: str):\n",
    "    natural_paths = glob.glob('*.csv', root_dir=natural_folder)\n",
    "    synthetic_paths = glob.glob('*.csv', root_dir=synthetic_folder)\n",
    "\n",
    "    natural_paths = [os.path.join(natural_folder, path) for path in natural_paths]\n",
    "    synthetic_paths = [os.path.join(synthetic_folder, path) for path in synthetic_paths]\n",
    "\n",
    "    natural = pd.concat([pd.read_csv(path) for path in natural_paths], ignore_index=True)\n",
    "    natural['source'] = 'HUMAN'\n",
    "\n",
    "    synthetic = pd.concat([pd.read_csv(path) for path in synthetic_paths], ignore_index=True)\n",
    "    synthetic['source'] = 'GPT'\n",
    "\n",
    "    return pd.concat([natural, synthetic], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = merge_datasets(natural_folder='./data/data_with_tags/natural/', synthetic_folder='./data/data_with_tags/synthetic/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('./data/data_with_tags/fine-tune-test-data-test-2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
